---
title: "Impact of Severe Weather Events Across the United States between 1950 and 2011"
author: "Lawrence C. Chen"
date: "Friday, May 15, 2015"
output:
  html_document:
    keep_md: yes
    number_sections: yes
    toc: yes
    toc_depth: 3
---

# Synopsis

In 2005, the destructive prower of [Hurricane Katrina](http://en.wikipedia.org/wiki/Hurricane_Katrina) brought an estimated $108 billion damage to properties along central Florida and
Texas and significant number of human casuality in New Orleans, Louisiana.  

The [Great Flood of 1993](http://en.wikipedia.org/wiki/Great_Flood_of_1993) along
the Mississippi and Missouri river last nearly 200 days with damages estimated
between $15-20 billion.  

According to the [NOAA Storm Database](http://www.ncdc.noaa.gov/stormevents/ftp.jsp),
states in the Southeast and Southwest are most suspectible to severe storm and
and flood related weather resulting in billions of properties damages. 

* What types of event occurs most frequent

* What types of event causes the greatest amount of damage

* Across the United States, which types of events (as indicated in the EVTYPE
variable) are most harmful with respect to population health?

* Across the United States, which types of events have the greatest economic
consequences?

# Loading and preprocessing the raw data

```{r download and sample data, results='hide'}
url <- 'https://d396qusza40orc.cloudfront.net/repdata%2Fdata%2FStormData.csv.bz2'
filename <- 'repdata-data-StormData.csv.bz2'

#check if the data file is available
if (!file.exists(filename)) {
    download.file(url = url, destfile = filename)
}

sample <- read.csv(filename, nrows = 100)

colClasses <- vapply(sample, class, 'character')
ch <- grep('(logical|integer|factor)', colClasses)
colClasses[ch] <- 'character'
```


```{r data, cache=TRUE, message=FALSE}
library(data.table)

dts <- as.data.table(read.csv(filename, na.strings = '', colClasses = colClasses))
```

This data set consists of `r dim(dts)[1]` observations and `r dim(dts)[2]`
variables.  The first 6 rows of the data are as below:

```{r, echo=FALSE, message=FALSE}
library(data.table)

head(dts)
```

Columns crucial to the analysis are:

* STATE (2-letter state designation, including the US Territories)
* BGN_DATE (date which the event occurred)
* EVTYPE (type of event)
* FATALITIES (no. of human fatalities)
* INJURIES (no. of human injuries)
* PROPDMG (dollar amount of property damage)
* PROPDMGEXP (magnitude of amount in thousand (K), million (M), or billion (B))
* CROPDMG (dollar amount of crop damage)
* CROPDMGEXP (magnitude of amount in thousand (K), million (M), or billion (B))

## Data cleansing
```{r colnames, results='hide'}
library(data.table)

setnames(dts, c(1, 35), c('STATE_FIPS', 'LONGITUDE_E'))

dts[, ':='(STATE_FIPS =as.integer(STATE_FIPS),
           BGN_DATE   =as.IDate(BGN_DATE, '%m/%d/%Y'),
           EVTYPE     =tolower(EVTYPE),           
           REFNUM     =as.integer(REFNUM))]

obs.total <- dim(dts)[1]
obs.dmg <- dim(dts[PROPDMG > 0 | CROPDMG > 0])[1]
obs.badexp <- dts[PROPDMG > 0 | CROPDMG > 0,
                  sum((!grepl('[KMBkmb]', PROPDMGEXP) |
                           !grepl('[KBMkbm]', CROPDMGEXP)))]

```

Of `r obs.total` total observations (obs), `r obs.dmg` obs. contain property or
crop damage estimates.  However, `r round(obs.badexp / obs.dmg * 100, 2)` percent
of which do not have the proper magnitude designations: K, M or B. In those
instances, thousand (K) is assumed. If magnitude is specified as one of the number
between 1 to 9, the number is used as the exponent of 10.

### Adjusting property and crop damages

```{r damage calc, results='hide'}
dmgAmt <- function(value, magnitude = 'K') {
    #returns dollar value in mulitple of thousands.
    #If magnitude is not one of 'K', 'M' or 'B', assume 'K'.

    #assumes value and magnitude are both equal length

    val <- numeric(length(value))

    for (i in 1:length(value)) {
        mag <- toupper(magnitude[i])
        
        if (grepl('[1-9]', mag)) {
            val[i] <- value[i] * 10^(as.numeric(mag) - 3)
        } else if (!grepl('[MB]', mag) | is.na(mag)) {
            val[i] <- value[i]
        } else if (mag == 'M') {
            val[i] <- value[i] * 1e3
        } else if (mag == 'B') {
            val[i] <- value[i] * 1e6
        }
    }

    val
}

dts[PROPDMG > 0, PropDmg:=dmgAmt(PROPDMG, PROPDMGEXP)]

dts[CROPDMG > 0, CropDmg:=dmgAmt(CROPDMG, CROPDMGEXP)]

dts[is.na(PropDmg), PropDmg:=0]
dts[is.na(CropDmg), CropDmg:=0]
```

### Summary from adjusting the affected damage estimates

```{r adjust magnitude}
bad.propexp <- dts[, PROPDMG > 0 & grepl('[^KBMkmb]', PROPDMGEXP)]

bad.cropexp <- dts[, CROPDMG > 0 & grepl('[^KMBkmb]', CROPDMGEXP)]

propdmg <- dts[bad.propexp, .(PROPDMG=sum(PROPDMG),
                              PropDmg=sum(PropDmg))]
cropdmg <- dts[bad.cropexp, .(CROPDMG=sum(CROPDMG),
                              CropDmg=sum(CropDmg))]

dmg.summary <- cbind(t(propdmg), t(cropdmg))
colnames(dmg.summary) <- c('Properties', 'Crops')
rownames(dmg.summary) <- c('Original', 'Adjusted')

dmg.summary
```

The adjustment resulted in `r ceiling(dmg.summary[2,1] / dmg.summary[1,1]) - 1`-fold
improvement in property damage estimates, or $`r ceiling((dmg.summary[2,1] - dmg.summary[1,1]) / 1e3)`
billions.


### Summary of damage estimates by states

```{r damage tally}
dmg.summary <- dts[, .('Prop Damage'=sum(PropDmg, na.rm = T),
                       'Crop Damage'=sum(CropDmg, na.rm = T),
                       Injuries=sum(INJURIES, na.rm = T),
                       Fatalitites=sum(FATALITIES, na.rm = T)),
                   by = STATE]

zero.dmg <- which(rowSums(dmg.summary[, c(2:5), with=F]) == 0)

head(dmg.summary)
```

And those states have no reported human injuries, fatalities, property or crop
damages are: `r dmg.summary$STATE[zero.dmg]`

### 2005 New Year's Eve Napa Valley Flood
The initial data exploration revealed that property and crop damage estimates of
the flood in Napa 2006 (REFNUM 605943) were erroneous by several order of
magnitude.

```{r napa original estimates}
#REFNUM 605943 original estimates
dts[REFNUM == 605943, c('PROPDMG', 'PROPDMGEXP', 'CROPDMG', 'CROPDMGEXP'), with=F]
```

 These values were corrected using estimated located in this
[News Article](http://napavalleyregister.com/news/local/napa-flood-damage-estimate-drops-to-million/article_bc2b3e27-1a7e-5886-ada4-6da209b4a5db.html 'Title'). 
[News footage](https://youtu.be/OQ5S86IAsn4) and additional information from
usgs.gov can be found [here](http://pubs.usgs.gov/of/2006/1182/pdf/ofr2006-1182.pdf).

```{r napa, results='hide'}
#REFNUM 605943 correction
dts[REFNUM == 605943, ':='(PROPDMG=13.75,
                           PROPDMGEXP='M',
                           CROPDMG=(114.2 - 13.75),
                           CROPDMGEXP='M')]
```



## Dimensionality reduction (feature extraction) of EVTYPE

```{r event types}
# Permitted Storm Data Events
evlist <- read.csv('EventTable.csv', na.strings='')

# Event types from the data set
evtype <- sort(dts[, unique(EVTYPE)])

head(evtype, 30)

```

Many of the EVTYPE (event types; `r length(evtype)` total) in the data set do
not match the [official list](http://www.ncdc.noaa.gov/stormevents/pd01016005curr.pdf)
of `r dim(evlist)[1]` event names. Best attempt was applied to semi-manually
pattern-match EVTYPE to the official list of events.

```
## EVTYPE text pattern 
#VOLCANIC ASH
dts[grepl('^vo', EVTYPE), EVTYPE:='Volcanic Ash']

#HAIL
dts[grepl('^hail', EVTYPE), EVTYPE:='Hail']

#HIGH WIND
dts[grepl('^high wind', EVTYPE), EVTYPE:='High Wind']

#HURRICANE
dts[grepl('^hurricane', EVTYPE), EVTYPE:='Hurricane (Typhoon)']
```
... however matching EVTYPE and maintaining the code this way gets tedious quickly.
A pragmatic alternative is to alphabetically sort and save the list
unique EVTYPE to a csv file first.

```{r write foo.csv}
write.table(sort(unique(dts$EVTYPE)),
            'foobar.csv', sep =',', col.names = c('EVTYPE'), row.names = F)
```

Then the official names of events are assigned to the adjacent column to produce
the a EVTYPE-to-EventName mapping file found [here](https://github.com/lawrence009/datasciencecoursera/blob/master/repdata/peer2/EventMapping.csv).

```{r merge evmap}
evmap <- read.csv('EventMapping.csv', na.strings = '', colClasses = 'character')[2:3]

#fraction of EVTYPE that is not mapped
sum(is.na(evmap)) / length(evtype)

#append the official event name
dts <- merge(dts, evmap, by = 'EVTYPE', all.x = T)

head(dts[, c('EVTYPE', 'EventName'), with=F])
```

Where ambiguous, EVTYPE is cross-reference against REMARKS.

```{r grepl EVTYPE REMARK, results='hide'}
## EVTYPE is summary...

#HUNDERSTORM WIND
dts[grepl('^summary', EVTYPE) & grepl('[Tt]hunderstorm.*[Ww]ind', REMARKS), EventName:='Thunderstorm Wind']

#HAIL
dts[grepl('^summary', EVTYPE) & grepl('[Hh]ail', REMARKS), EventName:='Hail']

#FLASH FLOOD
dts[grepl('^summary', EVTYPE) & grepl('[Ff]lash flood', REMARKS), EventName:='Flood']

#LIGHTNING
dts[grepl('^summary', EVTYPE) & grepl('[Ll]ightning', REMARKS), EventName:='Lightning']

#BLIZZARD
dts[grepl('^summary', EVTYPE) & grepl('[Bb]lizzard', REMARKS), EventName:='Blizzard']

```



### Assessment of EVTYPE extraction (map and replace) to EventName

```{r mapping assessment}

sum(is.na(dts$EventName)) / length(dts$EventName)

dmg.notmapped <- dts[is.na(EventName),
                    .(fatalities=sum(FATALITIES),
                      injuries=sum(INJURIES),
                      proddmg=sum(PropDmg),
                      cropdmg=sum(CropDmg))]


dmg.mapped <- dts[!is.na(EventName),
                  .(fatalities=sum(FATALITIES),
                    injuries=sum(INJURIES),
                    proddmg=sum(PropDmg),
                    cropdmg=sum(CropDmg))]


dmg.summary <- as.matrix(rbind(dmg.mapped, dmg.notmapped))
rownames(dmg.summary) <- c('mapped', 'not mapped')

dmg.summary

dmg.notmapped / dmg.mapped
```

**Conclusion:** The process is effective eventhough only `r round(sum(is.na(evmap)) / length(evtype) * 100)`
percent of the EVTYPE is mapped.

### Key weather elements

Weather events can also be further reduced using `r length(unique(evlist$Element1))`
key weather elements to enhance the analysis: `r sort(unique(evlist$Element1))`.

```{r elements}
summary(evlist) # list of official event names and the associated key elements

dts <- merge(dts, evlist, by = 'EventName', all.x = T)

summary(dts[, c('EVTYPE', 'EventName', 'Element1', 'Element2'), with=F])

```

The majority of the weather events are linked to key weather elements: water, ice and wind. 

### Bureau of Economic Analysis (BEA)
When comparing economic data, BEA further defines the states in the US into
[8 regions](http://bea.gov/regional/docs/regions.cfm):

1. New England
2. Mideast
3. Great Lakes
4. Plains
5. Southeast
6. Southwest
7. Rocky Mountain
8. Far West

The region-to-state mapping extracted from BEA can be found [here](https://github.com/lawrence009/datasciencecoursera/blob/master/repdata/peer2/BureauOfEconomicAnalysisRegions.csv).

```{r BEA US regions}
regions <- read.csv('BureauOfEconomicAnalysisRegions.csv')[c(3, 1)]
colnames(regions)[1] <- 'STATE'

summary(regions)

dts <- merge(dts, regions, by = 'STATE', all.x = T)

summary(dts[, c('STATE', 'Region'), with=F])
```

# Analysis

```{r event freq}
library(knitr)

colnames(dts)

dts <- dts[, c(1:2, 5, 24:29, 38:43), with=F]

colnames(dts)


table(dts[(PropDmg + CropDmg) > 0, c('Region', 'Element1'), with=F]) -> dmg.freq
table(dts[FATALITIES > 0, c('Region', 'Element1'), with=F]) -> ftl.freq
table(dts[INJURIES   > 0, c('Region', 'Element1'), with=F]) -> inj.freq


freq.summary <- function(x) {
    # returns a data frame summarizing the frequency of element by region
    # x is a subset of dts that resembles y
    
    # distribution of event type by region
    y <- table(dts[, c('Region', 'Element1'), with=F])

    z <- merge(x, y, by=c('Region', 'Element1'))

    z$ratio <- z$Freq.x / z$Freq.y

    with(z, z[order(Region, -Freq.x, -ratio), ])
}


# percent of event with human fatalities
kable(freq.summary(ftl.freq), row.names = F, digits=2)

# percent of event with human injuries
kable(freq.summary(inj.freq), row.names = F, digits=2)

# percent of event with property or crop damange
kable(freq.summary(dmg.freq), row.names = F, digits=2)
 
```

@@

# Results

```{r fig1 humn}
library(lattice)

#casuality mulitplier
lamda <- 100

dts[, ':='(econ  =(PropDmg + CropDmg),
           humn  =(FATALITIES*lamda + INJURIES),
           year  =as.factor(year(BGN_DATE)),
           month =as.factor(month(BGN_DATE)))] -> dts #supress print; not copied

bwplot(humn~Element1 | Region , data = dts[humn > 0],
       par.strip.text = list(cex = 0.8),
       main = 'Human Casulities (adjusted for fatalities)',
       ylab = 'Fatalities and Injuries',
       scales = list(cex = 0.8,
                     x = list(rot = 90),
                     y = list(log = 10)))


```


```{r fig2 econ}

bwplot(econ~Element1 | Region, data = dts[econ > 0],
       par.strip.text = list(cex = 0.8),
       main = 'Economic Impact',
       ylab = 'Property and Crop Damages (dollar)',
       scales = list(cex = 0.8,
                     x = list(rot = 90),
                     y = list(log = 10)))


```


***

# Assignment Instructions 

The basic goal of this assignment is to explore the NOAA Storm Database and answer some basic questions about severe weather events. You must use the database to answer the questions below and show the code for your entire analysis. Your analysis can consist of tables, figures, or other summaries. You may use any R package you want to support your analysis.

Questions

Your data analysis must address the following questions:

* Across the United States, which types of events (as indicated in the EVTYPE variable) are most harmful with respect to population health?

* Across the United States, which types of events have the greatest economic consequences?

Consider writing your report as if it were to be read by a government or municipal manager who might be responsible for preparing for severe weather events and will need to prioritize resources for different types of events. However, there is no need to make any specific recommendations in your report.


##Requirements

###Document Layout

Language: Your document should be written in English.

Title: Your document should have a title that briefly summarizes your data analysis

Synopsis: Immediately after the title, there should be a synopsis which describes and summarizes your analysis in at most 10 complete sentences.

There should be a section titled Data Processing which describes (in words and code) how the data were loaded into R and processed for analysis. In particular, your analysis must start from the raw CSV file containing the data. You cannot do any preprocessing outside the document. If preprocessing is time-consuming you may consider using the cache = TRUE option for certain code chunks.

There should be a section titled Results in which your results are presented.

You may have other sections in your analysis, but Data Processing and Results are required.

The analysis document must have at least one figure containing a plot.

Your analysis must have no more than three figures. Figures may have multiple plots in them (i.e. panel plots), but there cannot be more than three figures total.

You must show all your code for the work in your analysis document. This may make the document a bit verbose, but that is okay. In general, you should ensure that echo = TRUE for every code chunk (this is the default setting in knitr).

